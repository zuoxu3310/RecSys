{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENT BASED MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, we have a service (like Youtbe, Netflix or whatever) and we have some new users. We want to recommend our users something they would like, but those users are new, so we know nothing about them. How to build recommender system in such situation? Another question is how to score and recommend new items with known features but without known ratings. \n",
    "\n",
    "The answer is simple: we have items (videos, movies, ect). Each item has some features and each item has its own rating (calculated based on the grades users or critics put). We want to recommend items with higher rating to our newbies. Below there are three ways how to do that.\n",
    "\n",
    "But firstly, let me show the datsets we are going to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the preprocessed dataset [here](https://drive.google.com/drive/folders/1YPqWaZYW2axz91YKkaM7j_xPqF5rSVx2?usp=drive_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFI-ui8K6uZN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import norm as sparse_norm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look on our *reviews* dataset. In this dataset we need only four columns with titles *user_id*, *anime_id*, *favorites_anime* and *score*.\n",
    "\n",
    "- ***user_id*** User profile name. Some profile names are literally insane =)\n",
    "\n",
    "- ***anime_id*** The mapping of anime titles column onto integers.\n",
    "\n",
    "- ***score*** grade, how the user with the specific *user_id* evaluated the anime with specific *anime_id*.\n",
    "\n",
    "-  ***favorites_anime*** List of animes the user with the specific *user_id*  considered as his favourites. **ATTENTION** the animes from this list may not occur in user history (animes user graded). User may have mentioned some animes as favourites, but did not graded them. Automatically, we will consider that the score of favorite animes is **10**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_COL = \"user_id\"\n",
    "ITEM_COL = \"anime_id\"\n",
    "RELEVANCE_COL = \"score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./anime_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data = pd.read_csv(os.path.join(base_path, 'reviews.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you remember what is the difference between explicit and implicit feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_info(log, user_id='user_id', item_id='item_id'):\n",
    "    print(f'Num reviews = {log.shape[0]},\\nnum users = {log[user_id].nunique()},\\nnum items = {log[item_id].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_log_info(review_data, user_id=USER_COL, item_id=ITEM_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_data[RELEVANCE_COL].plot.hist(bins=10, figsize=(10, 5), title='Scores distibution from user reviews');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the distribution of ratings change if we have, for example, a marketplace data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_and_plot(df, group_by_name, rating_col_name, quantile=0.99, title=''):\n",
    "    grouped = df.groupby(group_by_name)[rating_col_name].count()\n",
    "    print(grouped.describe(percentiles=[0.05, .25, .5, .75, 0.95]))\n",
    "    grouped[grouped < grouped.quantile(quantile)].plot(kind='hist', bins=20, figsize=(10, 5), title=title)\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_and_plot(review_data, group_by_name=USER_COL, rating_col_name=RELEVANCE_COL, quantile=0.99, title='Num reviews per user');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_and_plot(review_data, group_by_name=ITEM_COL, rating_col_name=RELEVANCE_COL, quantile=0.99, title='Num reviews per anime');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many animes could we reliably recommend using popularity-based methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main columns for us are:\n",
    "\n",
    "- ***anime_id*** - the same id we have in the table above.\n",
    "\n",
    "- ***synopsis*** - the description of the anime with a specific **anime_id**\n",
    "\n",
    "- ***score*** - average score over all the grades users or critics  put to the corresponding anime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data = pd.read_csv(os.path.join(base_path, 'animes.gz'), na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data['genre'].str.strip().str.split(\", \").explode().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data.shape # 7636 in reviews, the rest won't be covered by the popularity-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data[RELEVANCE_COL].plot.hist(bins=10, title='Score distibution from item features', figsize=(10, 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(anime_data[RELEVANCE_COL] > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anime_data = (\n",
    "    anime_data[['anime_id', 'score']]\n",
    "    .assign(tokens=anime_data[['title', 'genre', 'synopsis']].apply('; '.join, axis=1))\n",
    "    .set_index('anime_id')\n",
    "    # ['tokens']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anime_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can we do with these tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains additional information about users, including their favorite anime.\n",
    "\n",
    "- ***user_id*** - the same user id as in the reviews table.\n",
    "\n",
    "- ***gender*** - user gender\n",
    "\n",
    "- ***birthday*** - user birthday\n",
    "\n",
    "- ***favorites_anime*** - a list of user favorites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data = pd.read_csv(os.path.join(base_path, 'profiles.gz'), converters={'favorites_anime': ast.literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    profile_data['favorites_anime']\n",
    "    .apply(len)\n",
    "    .value_counts(sort=False).sort_index()#.cumsum()\n",
    "    .plot.bar(\n",
    "        logy=True,\n",
    "        rot=0,\n",
    "        xlabel='# favorites',\n",
    "        ylabel='frequency',\n",
    "        title='Amount of favorites in user profiles',\n",
    "        figsize=(10, 5)\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Favorites ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites_data = profile_data.set_index('user_id')['favorites_anime']\n",
    "favorites_scores = pd.merge(\n",
    "    favorites_data.explode().rename('anime_id').reset_index(),\n",
    "    review_data[['user_id', 'anime_id', 'score']],\n",
    "    on = ['user_id', 'anime_id'],\n",
    "    how = 'left'\n",
    ")['score']\n",
    "\n",
    "print(f'Fraction of favorites without the rating: {favorites_scores.isnull().mean():.0%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Most of the favorites do not have ratings info.\n",
    "- For the sake of evaluation in this excercise, we will make two assumptions:\n",
    "  - favorites are of the highest interest to users,\n",
    "  - other animes that users rate highly should be somewhat similar to their favorites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to construct test set we use the users who have some anime in their favourites list.\n",
    "- For these users we take **n_pairs** animes a user liked and **n_pairs** animes the user disliked from their anime reviews.\n",
    "  - We assume the user liked anime if he put the grade no less than some *score_cutoff* value.\n",
    "- The quality of predictions is then evaluated by comparing how close the liked animes to the favorites based on the predicted scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E60Q3lM1TSx3"
   },
   "outputs": [],
   "source": [
    "def get_test_pairs(reviews, favorites, n_pairs, score_cutoff, seed):\n",
    "    '''\n",
    "    Construct a dataset consisting of pairs of liked and disliked animes. The likes and dislikes\n",
    "    are defined by the ratings value: everything below threshold is a dislike, the rest are likes.\n",
    "        \n",
    "    The function ensures that the amount of likes and dislikes is the same per each user in data.\n",
    "    The users that do not contain enough likes or dislikes are discarded from the result.\n",
    "    The result is to be used for evaluating the quality of recommendations by some algorithms.\n",
    "    Hence, user favorites are excluded to ensure that there is no trivial solution.\n",
    "    '''\n",
    "    rng = np.random.default_rng(seed)\n",
    "    def strict_sample_no_favs(series):\n",
    "        # sample `n_pairs` elements from `series`, if not enough data - return empty list,\n",
    "        # discard favorites, otherwise the evaluation on test pairs against favorites makes no sense\n",
    "        above_cutoff, user_id = series.name\n",
    "        allowed_items = np.setdiff1d(series.values, favorites.loc[user_id])\n",
    "        return rng.choice(allowed_items, n_pairs, replace=False) if len(allowed_items)>=n_pairs else []\n",
    "    \n",
    "    test_pairs = (\n",
    "        reviews\n",
    "         # split by likes and dislikes, group by users\n",
    "        .groupby([(reviews[\"score\"] >= score_cutoff), 'user_id'])\n",
    "        # sample `n_pairs` items (both likes and dislikes), disregard user favorites\n",
    "        ['anime_id'].apply(strict_sample_no_favs)\n",
    "         # disregard users that have not enough items\n",
    "        .loc[lambda x: x.apply(len) > 0]\n",
    "         # make two columns of likes and dislikes\n",
    "        .unstack('score')\n",
    "        # ensure each user has both likes and dislikes\n",
    "        .dropna()\n",
    "         # rename by rule `score >= score_cutoff`\n",
    "        .rename(columns={False: 'dislikes', True: 'likes'})\n",
    "    )\n",
    "    return test_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate the training data by excluding animes from the pairs of likes and dislikes contained in the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QU5IadBOFvVx"
   },
   "outputs": [],
   "source": [
    "def split_anime_train_test_data(reviews, favorites, anime, *, n_pairs=3, score_cutoff=5, seed=0):\n",
    "    '''\n",
    "    Function to construct train dataset. It deletes animes that occured in the test set\n",
    "    to prevent information leakage from test to train.\n",
    "    '''\n",
    "    test_data = get_test_pairs(reviews, favorites, n_pairs, score_cutoff, seed)\n",
    "    test_anime_set = (\n",
    "        test_data\n",
    "        .melt(value_name='animes') # reshape 2 columns into signle column\n",
    "        ['animes'].explode() # flatten all lists into a single long 1d array\n",
    "        .unique() # get only unique values\n",
    "    )\n",
    "    train_data = (\n",
    "        anime\n",
    "        # only use known score - to be used as the prediction target,\n",
    "        # also prevent test data leaks into training\n",
    "        .query('score > 0 and anime_id not in @test_anime_set')\n",
    "        # combine several text fields into a unified feature view\n",
    "        .assign(tokens = lambda x: x[['title', 'genre', 'synopsis']].apply('; '.join, axis=1))\n",
    "        # only take necessary fields\n",
    "        .loc[:, ['anime_id', 'tokens', 'score']]\n",
    "    )\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MCyJAnMyF0Gd"
   },
   "outputs": [],
   "source": [
    "anime_train, anime_test = split_anime_train_test_data(\n",
    "    review_data, favorites_data, anime_data, score_cutoff=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to evaluate classification/regression metrics\n",
    "\n",
    "test_animes = (anime_test\n",
    "        .melt(value_name='animes') # reshape 2 columns into signle column\n",
    "        ['animes'].explode() # flatten all lists into a single long 1d array\n",
    "        .unique()) # get only unique values\n",
    "\n",
    "anime_test_animes = anime_data.query('score > 0 and anime_id in @test_animes') \\\n",
    "        .assign(tokens = lambda x: x[['title', 'genre', 'synopsis']].apply('; '.join, axis=1)) \\\n",
    "        .loc[:, ['anime_id', 'tokens', 'score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data.query('score > 0').shape, anime_train.shape[0] + anime_test_animes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_test_animes.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_and_plot(review_data.query('user_id in @anime_test.index.values'), group_by_name=USER_COL, rating_col_name=ITEM_COL, quantile=0.99, title='Num reviews per test user');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to build a personalized models for those users in your HW. Do you have enough data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D97CoFjUdjgJ"
   },
   "source": [
    "# Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz7dE2lFf5gN"
   },
   "source": [
    "<!-- ## PURE CONTENT BASED MODELS\n",
    "\n",
    "We will explore application of regression and classification approaches.\n",
    " -->\n",
    "In regression task (in its straightforward implementation) we want to predict average score based on any parameter (parameters) of anime. Here we will use binary vectors of synopsis of each anime\n",
    "\n",
    "$$\n",
    "S = W * x +W_0\n",
    "$$\n",
    "\n",
    "where $x$ - text representation $S$ -predicted scores, $W_0$ -bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case we need any representation of textual information. In first we will use a binary representation (if the word presents in text) and than will apply TFIDF representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Here we need to predict anime's rating. For this purpose we are going to use ***regression model***. The simplest regression model is ***Linear regression***.\n",
    "\n",
    "\n",
    "$$\n",
    "S = W * x +W_0\n",
    "$$\n",
    "\n",
    "\n",
    "Where **S** is predicted scores, **x** is our text representation and **W** is model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see a simple pipeline for this task. Later we're going to extend it during our course.\n",
    "\n",
    "- ***build_cb_model*** - this function constructs the model we need. As a rating predictor we use Linear regression.  The entries of this function is model config, trainset_description and trainset which we build above\n",
    "\n",
    "- ***generate_features*** - applies CountVectorizer or TfidfVectorizer for items' descriptions\n",
    " \n",
    "- ***cb_model_scoring*** - provides the scores for  the entry set of data. This function uses parameters which were defined in ***build_cb_model*** \n",
    "\n",
    "- ***cb_config*** - defines parameters of the model\n",
    "\n",
    "- ***transform_predict*** - unified function to generate recommendations\n",
    "\n",
    "- ***anime_description*** - maps names of features we use in ***build_cb_model***/***cb_model_scoring*** to column names, used for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert sparse matrix to dense np array to apply standard scaler with mean.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bqwjwe8DLdzr"
   },
   "outputs": [],
   "source": [
    "def build_cb_model(config, trainset, trainset_description, logistic=False, binary_vectorizer=True):\n",
    "    \"\"\"\n",
    "    Config and fit cb model\n",
    "    \"\"\"\n",
    "    feature_matrix, word_vectorizer = generate_features(config, trainset, trainset_description, binary_vectorizer)\n",
    "    if logistic:\n",
    "        regressor = LogisticRegression\n",
    "    elif 'alpha' in config['model']:\n",
    "        regressor = Ridge\n",
    "    else:\n",
    "        regressor = LinearRegression\n",
    "    target_column = trainset_description['feedback']\n",
    "    model = regressor(**config['model']).fit(feature_matrix, trainset[target_column])\n",
    "    return model, word_vectorizer\n",
    "\n",
    "def generate_features(config, trainset, trainset_description, binary_vectorizer):\n",
    "    \"\"\"\n",
    "    Config and fit text vectorizer\n",
    "    \"\"\"\n",
    "    if binary_vectorizer:\n",
    "        word_vectorizer = CountVectorizer(**config['vectorizer']['binary'])\n",
    "    else:\n",
    "        word_vectorizer = Pipeline([(\"tfidf\", TfidfVectorizer(**config['vectorizer']['tfidf'])), \n",
    "                                    ('dense', DenseTransformer()), \n",
    "                                    (\"scaler\", StandardScaler())])\n",
    "    features_column = trainset_description['item_features']\n",
    "    feature_matrix = word_vectorizer.fit_transform(trainset[features_column])\n",
    "    return feature_matrix, word_vectorizer\n",
    "\n",
    "\n",
    "def transform_predict(params, tokens):\n",
    "    \"\"\"\n",
    "    Get recommendations from either classification or regression model\n",
    "    \"\"\"\n",
    "    model, word_vectorizer = params\n",
    "    tokens_encoded = word_vectorizer.transform(tokens)\n",
    "    try: # handle classification models\n",
    "        predictor = model.predict_proba\n",
    "    except AttributeError:\n",
    "        predictor = model.predict\n",
    "    scores = predictor(tokens_encoded)\n",
    "    if scores.ndim > 1: # handle classification\n",
    "        scores = scores[:, 1] # take class 1\n",
    "    return scores\n",
    "\n",
    "def cb_model_scoring(params, testset, testset_description):\n",
    "    \"\"\"\n",
    "    Select necessary features and get recommendations with the fitted pipeline\n",
    "    \"\"\"\n",
    "    tokens = testset[testset_description['item_features']]\n",
    "    scores = transform_predict(params, tokens)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with the binary text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXTusBqZLdrX"
   },
   "outputs": [],
   "source": [
    "cb_config = {\n",
    "    \"model\": dict(),\n",
    "    \"vectorizer\":{\n",
    "        \"binary\": dict( # simple binary token encoder\n",
    "            min_df = 5,\n",
    "            max_df = 0.9,\n",
    "            strip_accents='unicode',\n",
    "            stop_words = 'english',\n",
    "            analyzer = 'word',\n",
    "            binary = True,\n",
    "        ),\n",
    "        \"tfidf\": dict( # TfIDF Vectorizer\n",
    "            min_df = 5,\n",
    "            max_df = 0.9,\n",
    "            strip_accents='unicode',\n",
    "            stop_words = 'english',\n",
    "            analyzer = 'word',\n",
    "            use_idf = True,\n",
    "            smooth_idf = True,\n",
    "            sublinear_tf = True,\n",
    "            binary = False,\n",
    "            norm=\"l2\",\n",
    "        ),\n",
    "    }\n",
    "}\n",
    "# we also define a general representation of our dataset\n",
    "anime_description = {\n",
    "    'feedback' : \"score\",\n",
    "    \"item_features\": \"tokens\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbIQVutcLdpB"
   },
   "outputs": [],
   "source": [
    "reg_params = build_cb_model(cb_config, anime_train, anime_description)\n",
    "reg_scores = cb_model_scoring(reg_params, anime_train, anime_description)\n",
    "reg_scores_test_anime = cb_model_scoring(reg_params, anime_test_animes, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reg_params[1].vocabulary_), anime_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any conserns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reg_params[0].coef_), np.linalg.norm(reg_params[0].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params[0].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: build a CountVectorizer + LinearRegression model without given functions, fit it with `anime_train` and get the scores for `anime_test_animes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple evaluation of our model\n",
    "\n",
    "Here we compare predicted scores to the ground truth using MAE and RMSE metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rmse_mae(predicted_scores, gt_scores):\n",
    "    rmse = np.sqrt(np.mean((predicted_scores - gt_scores)**2))\n",
    "    mae = np.mean(np.abs(predicted_scores - gt_scores))\n",
    "    print(f'{rmse=:.4f}, {mae=:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_train['score'].values, reg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_test_animes['score'].values, reg_scores_test_anime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "om74aDKBeGMN"
   },
   "source": [
    "## A little bit of analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the significant features of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f8tIiJnLdmL"
   },
   "outputs": [],
   "source": [
    "def top_idx(a, topk):\n",
    "    # idx of top-k with the biggest scores\n",
    "    parted = np.argpartition(a, -topk)[-topk:]\n",
    "    # idx of top-k sorted descending by relevance \n",
    "    return parted[np.argsort(-a[parted])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFoHIbxRNvi7"
   },
   "outputs": [],
   "source": [
    "def significant_features(params, topn=10, reverse=False):\n",
    "    reg, word_vectorizer = params\n",
    "    features_weights = reg.coef_.squeeze()\n",
    "    if reverse:\n",
    "        features_weights = -features_weights\n",
    "    top_features_idx = top_idx(features_weights, topn)\n",
    "    if isinstance(word_vectorizer, Pipeline):\n",
    "        word_vectorizer = word_vectorizer[0]\n",
    "    try: # handle API changes in different versions of sklearn\n",
    "        features = word_vectorizer.get_feature_names()\n",
    "    except AttributeError:\n",
    "        features = word_vectorizer.get_feature_names_out()\n",
    "\n",
    "    feature_scores = pd.DataFrame(\n",
    "        zip(np.array(features)[top_features_idx], np.array(features_weights)[top_features_idx]),\n",
    "        columns = ['feature', 'weight']\n",
    "    )\n",
    "    return feature_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "xps3UsRELdbn",
    "outputId": "2f7dc9b0-0f10-42f2-9d07-dc2731b6ae86"
   },
   "outputs": [],
   "source": [
    "# features with the most positive impact\n",
    "significant_features(reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HoTaW2hwN7Jq",
    "outputId": "af349aed-c6c3-49f2-d5f2-7f83147ba312"
   },
   "outputs": [],
   "source": [
    "# features with the most negative impact\n",
    "significant_features(reg_params, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_anime_name = \"Kara no Kyoukai\"\n",
    "the_anime_series = anime_data[anime_data['title'].str.contains(the_anime_name)].anime_id.unique()\n",
    "the_anime_reviews = review_data[(review_data['anime_id'].isin(the_anime_series))].copy()\n",
    "the_anime_reviews.loc[:, 'text'] = the_anime_reviews['text'].str.lower()\n",
    "ryougi = the_anime_reviews[the_anime_reviews['text'].str.contains(\"ryougi\")].shape[0]\n",
    "kokutou = the_anime_reviews[the_anime_reviews['text'].str.contains(\"kokutou\")].shape[0]\n",
    "both = the_anime_reviews[(the_anime_reviews['text'].str.contains(\"kokutou\")) \n",
    "       & (the_anime_reviews['text'].str.contains(\"ryougi\"))].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Num reviews for {the_anime_name} with {ryougi = } with {kokutou = }, with {both = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also gunsou (2-nd positive) keroro (6-th negative) is a name of an anime.\n",
    "\n",
    "#### What could be the reason? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression with the binary text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXTusBqZLdrX"
   },
   "outputs": [],
   "source": [
    "cb_config[\"model\"] = {\"alpha\": 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbIQVutcLdpB"
   },
   "outputs": [],
   "source": [
    "ridge_reg_params = build_cb_model(cb_config, anime_train, anime_description)\n",
    "ridge_reg_scores = cb_model_scoring(ridge_reg_params, anime_train, anime_description)\n",
    "ridge_reg_scores_test_anime = cb_model_scoring(ridge_reg_params, anime_test_animes, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(ridge_reg_params[0].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg_params[0].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_train['score'].values, ridge_reg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_test_animes['score'].values, ridge_reg_scores_test_anime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "xps3UsRELdbn",
    "outputId": "2f7dc9b0-0f10-42f2-9d07-dc2731b6ae86"
   },
   "outputs": [],
   "source": [
    "# features with the most positive impact\n",
    "significant_features(ridge_reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HoTaW2hwN7Jq",
    "outputId": "af349aed-c6c3-49f2-d5f2-7f83147ba312"
   },
   "outputs": [],
   "source": [
    "# features with the most negative impact\n",
    "significant_features(ridge_reg_params, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Think, does it make sense to use this kind of analysis of significant features in the case of non-binary feature matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-10, 20, 100)\n",
    "plt.hist(reg_scores_test_anime, bins=bins, alpha=0.5, label='linear');\n",
    "plt.hist(anime_test_animes['score'].values, bins=bins, alpha=0.5, label='gt');\n",
    "plt.hist(ridge_reg_scores_test_anime, bins=bins, alpha=0.5, label='ridge');\n",
    "\n",
    "plt.title(\"Distribution of ground truth and predicted test scores\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** \n",
    "\n",
    "Do not forget to regularize your models)\n",
    "\n",
    "During the cource we will move from content-based models to the models, build on top os users interactions with the items. Items could also be pretty similar and appear in groups, leading to multicollinearity. Regularization will appear as one of the important success factors for collaborative filtering models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, we don't have exact anime rating. How to understand whether our model is sane or not? Let's refresh all the info we have for users $u\\in\\mathcal{U}$.\n",
    "\n",
    "For each user we have:\n",
    "- predicted scores $r_i$ for a user $u$ likes ($i\\in\\mathcal{I}_u^+$)\n",
    "- predicted scores $r_j$ for a user $u$ dislikes ($j\\in\\mathcal{I}_u^-$)\n",
    "- predicted scores $r_k$ for user favourites ($k\\in\\mathcal{I}_u^f$)\n",
    "\n",
    "By construction, the set of user favorites is disjoint from the items in the test user preferences $(\\mathcal{I}_u^+ \\cup \\mathcal{I}_u^-)\\cap\\mathcal{I}_u^f=\\emptyset$   (see `get_test_pairs` function).\n",
    "\n",
    "Intuitively, the predicted scores on a user's favourite animes should be (on average) closer to the predicted scores on the user's likes rather than on dislikes. So, the evaluation is based on the following \"closeness-rate\" metric:\n",
    "$$\n",
    "CR = \\frac{1}{|\\mathcal{U}|}\\sum_{u\\in\\mathcal{U}}\\frac{1}{|\\mathcal{I^f_u}|}\\sum_{k\\in\\mathcal{I}_u^f}\\mathbb{I}\\left(\\text{dist}(k,\\mathcal{I_u^+})<\\text{dist}(k,\\mathcal{I_u^-})\\right),\n",
    "$$\n",
    "where $\\mathbb{I}(\\cdot)$ is an indicator function that returns 1 or 0 depending on whether its argument is true or not. The distances can be mesured, for example, simply as\n",
    "$$\n",
    "\\text{dist}(k,\\mathcal{I_u^+}) = \\frac{1}{|\\mathcal{I_u^+}|}{\\sum_{i\\in\\mathcal{I_u^+}}(r_i - r_k)^2}, \\\\\n",
    "\\text{dist}(k,\\mathcal{I_u^-}) = \\frac{1}{|\\mathcal{I_u^-}|}\\sum_{j\\in\\mathcal{I_{u}^-}}{(r_j - r_k)^2}.\n",
    "$$\n",
    "\n",
    "In short, *the deviation of the predicted scores on favorite items from the predcited scores of liked items must be lower then that of the disliked items*. Note that in our setup $|\\mathcal{I}_u^+|=|\\mathcal{I}_u^-|=n\\_pairs$.\n",
    "\n",
    "Think of other possible functions for the distance estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsreQWZeZ5BA"
   },
   "outputs": [],
   "source": [
    "def cb_model_evaluation(params, test_pairs, favorites, anime_features, distance_function):\n",
    "    positive_is_closer = []\n",
    "    for user_id, likes, dislikes in test_pairs[['likes', 'dislikes']].itertuples(name=None):\n",
    "        pos_distance, neg_distance = distance_function(user_id, likes, dislikes, params, favorites, anime_features)\n",
    "        positive_is_closer.append((pos_distance < neg_distance).mean())\n",
    "    return np.mean(positive_is_closer)\n",
    "\n",
    "\n",
    "def cb_distances(user_id, likes, dislikes, params, favorites, anime_features):\n",
    "    \"\"\"\n",
    "    Caclulate the distance between user's favorites and likes, \n",
    "    as well as between user's favorites and dislikes based on scores of a regression model\n",
    "    \"\"\"\n",
    "    user_favorites = favorites[user_id]\n",
    "    # (n_positives,)\n",
    "    scores_pos = transform_predict(params, anime_features.loc[likes, 'tokens'])\n",
    "    scores_neg = transform_predict(params, anime_features.loc[dislikes, 'tokens'])\n",
    "    # (n_favorites,)\n",
    "    scores_fav = transform_predict(params, anime_features.loc[user_favorites, 'tokens'])\n",
    "    # (n_positives, n_favorites) -> (n_favorites), mean distance between the favorite and all users' positives\n",
    "    pos_deviation = np.power(np.subtract.outer(scores_pos, scores_fav), 2).mean(axis=0)\n",
    "    neg_deviation = np.power(np.subtract.outer(scores_neg, scores_fav), 2).mean(axis=0)\n",
    "    return pos_deviation,neg_deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6UHJrNY0d8t1",
    "outputId": "e8dc4733-5f15-4f09-fba8-ce3946c7e051"
   },
   "outputs": [],
   "source": [
    "cb_model_evaluation(reg_params, anime_test, favorites_data, all_anime_data, cb_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model_evaluation(ridge_reg_params, anime_test, favorites_data, all_anime_data, cb_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this result good or bad? Let's compare it to the random prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random prediction baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPredictor:\n",
    "    def __init__(self, seed):\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def predict(self, tokens_encoded):\n",
    "        return self.rng.standard_normal(tokens_encoded.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_res = []\n",
    "n_tries = 5\n",
    "for seed in np.random.SeedSequence().generate_state(n_tries):\n",
    "    rnd_params = (RandomPredictor(seed), reg_params[1])\n",
    "    rnd_res.append(cb_model_evaluation(rnd_params, anime_test, favorites_data, all_anime_data, cb_distances))\n",
    "\n",
    "print(f'Random model result: {np.mean(rnd_res):.4f}+-{np.std(rnd_res):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression with the TF-IDF text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According the wikipedia TFIDF is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. We wll use TFIDF for this purpose.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF constructs the representation of the text as a multiplication of two statistics : \n",
    "\n",
    "- ***Term frequency***, wich shows how many times a specific word (term) $t$ happens in the document (synopsis in our case) $d$\n",
    "\n",
    "$$tf (t,f) = \\frac{f(t,d)}{\\sum_{t' \\in d}{f(t',d)}}$$\n",
    "\n",
    "\n",
    "- ***Inverse document frequency*** shows how common or rare the word (term) across all documents.\n",
    "\n",
    "$$idf(t, D) = \\log{\\frac{|D|}{|d \\in D; t\\in d |}}$$\n",
    "\n",
    "\n",
    "So the resulting metric is just a multiplication of $tf$ and $idf$. This metrics automatically provides more attention to the words that are more important for document claclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"alpha\": 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXTusBqZLdrX"
   },
   "outputs": [],
   "source": [
    "cb_config[\"model\"] = {\"alpha\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbIQVutcLdpB"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_reg_params = build_cb_model(cb_config, anime_train, anime_description, binary_vectorizer=False)\n",
    "tfidf_reg_scores = cb_model_scoring(tfidf_reg_params, anime_train, anime_description)\n",
    "tfidf_reg_scores_test_anime = cb_model_scoring(tfidf_reg_params, anime_test_animes, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reg_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reg_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sublinear TF causes negative scores\n",
    "np.min(tfidf_reg_params[1].transform(anime_test_animes['tokens'])), np.max(tfidf_reg_params[1].transform(anime_test_animes['tokens']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_reg_params[0].coef_), np.linalg.norm(tfidf_reg_params[0].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reg_params[0].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_train['score'].values, tfidf_reg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_test_animes['score'].values, tfidf_reg_scores_test_anime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression feature importance with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "xps3UsRELdbn",
    "outputId": "2f7dc9b0-0f10-42f2-9d07-dc2731b6ae86"
   },
   "outputs": [],
   "source": [
    "# features with the most positive impact\n",
    "significant_features(tfidf_reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HoTaW2hwN7Jq",
    "outputId": "af349aed-c6c3-49f2-d5f2-7f83147ba312"
   },
   "outputs": [],
   "source": [
    "# features with the most negative impact\n",
    "significant_features(tfidf_reg_params, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model_evaluation(tfidf_reg_params, anime_test, favorites_data, all_anime_data, cb_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"alpha\": 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXTusBqZLdrX"
   },
   "outputs": [],
   "source": [
    "cb_config[\"model\"] = {\"alpha\": 20000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbIQVutcLdpB"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tfidf_reg_params = build_cb_model(cb_config, anime_train, anime_description, binary_vectorizer=False)\n",
    "tfidf_reg_scores = cb_model_scoring(tfidf_reg_params, anime_train, anime_description)\n",
    "tfidf_reg_scores_test_anime = cb_model_scoring(tfidf_reg_params, anime_test_animes, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reg_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reg_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(tfidf_reg_params[1].transform(anime_test_animes['tokens']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_reg_params[0].coef_), np.linalg.norm(tfidf_reg_params[0].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_reg_params[0].intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_train['score'].values, tfidf_reg_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_rmse_mae(anime_test_animes['score'].values, tfidf_reg_scores_test_anime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge regression feature importance with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "xps3UsRELdbn",
    "outputId": "2f7dc9b0-0f10-42f2-9d07-dc2731b6ae86"
   },
   "outputs": [],
   "source": [
    "# features with the most positive impact\n",
    "significant_features(tfidf_reg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HoTaW2hwN7Jq",
    "outputId": "af349aed-c6c3-49f2-d5f2-7f83147ba312"
   },
   "outputs": [],
   "source": [
    "# features with the most negative impact\n",
    "significant_features(tfidf_reg_params, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model_evaluation(tfidf_reg_params, anime_test, favorites_data, all_anime_data, cb_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shap values for Ridge regression with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_config[\"model\"] = {\"alpha\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(**cb_config[\"vectorizer\"][\"tfidf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectorized = tf_idf.fit_transform(anime_train[\"tokens\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tf_idf = Ridge(**cb_config[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_tf_idf.fit(train_vectorized, anime_train[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf.vocabulary_[\"music\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_tf_idf = ridge_tf_idf.predict(train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.explainers.Linear(ridge_tf_idf, train_vectorized)\n",
    "shap_values = explainer(train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add back the feature names stripped by the TfidfVectorizer\n",
    "for word,idx in tf_idf.vocabulary_.items():\n",
    "    shap_values.feature_names[idx] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0], max_display=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MCKd6AVOTph"
   },
   "source": [
    "# Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cv0mpSaxOTe9"
   },
   "source": [
    "- Your task is to turn regression model into classification model.\n",
    "- Use `LogisitcRegression` class from `sklearn` fro this.\n",
    "- adopt previously derived functions for the new approach.\n",
    "\n",
    "- try using more than 2 classes (i.e., not just binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2IqwiPqpOTXT"
   },
   "source": [
    "In classification task we predict any category (label). Here we will predict whether the  anime can be recommended or not. We will do it in simple way. For example LogReg\n",
    "\n",
    "\n",
    "$$\n",
    "y_{pred} = \\frac{1}{1+e^{W*x +W_0}}\n",
    "$$\n",
    "\n",
    "But first of all we need to label our dataset. Let's take a look at the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "HjZu4rYJOfTP",
    "outputId": "c6d5b066-1262-4dd1-f3fa-92e8082ccb1d"
   },
   "outputs": [],
   "source": [
    "anime_train.query('score>0').score.hist(bins=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mjpZlG6OhzB",
    "outputId": "c8f1fd71-7728-46c9-ae36-716b65dfcdc1"
   },
   "outputs": [],
   "source": [
    "anime_train.query('score>0').score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1OyTb8UOTQM"
   },
   "source": [
    "The histogram shows that the mean of our average score distribution is a little bit disolased.\n",
    "So we can take its mean as a borderline and say, that all the animes with average score < 6.24 are lame,  and all the animes with scores above this value are lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What kind of interactions data could we get and how could we transform the target to get binary targets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV7_WEO0O1Em"
   },
   "outputs": [],
   "source": [
    "def assign_labels(data, cutoff):\n",
    "    '''Add the binary `recommend` field based on the score cutoff value'''\n",
    "    labeled_df = data.assign(\n",
    "        recommend = lambda x: x['score'].ge(cutoff).astype(int)\n",
    "    )\n",
    "    return labeled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_config[\"model\"] = {\"C\": 0.1, \n",
    "                      \"max_iter\": 1000, \n",
    "                      \"class_weight\": \"balanced\"\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cutoff = 6.24\n",
    "\n",
    "gt_label = assign_labels(anime_train, score_cutoff)\n",
    "gt_label[\"recommend\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAbkP0c_O8tw"
   },
   "outputs": [],
   "source": [
    "logreg_params = build_cb_model(\n",
    "    cb_config,\n",
    "    # extend dataset with class labels\n",
    "    gt_label,\n",
    "     # target 0/1 instead of raw scores\n",
    "    {**anime_description, **{'feedback': 'recommend'}},\n",
    "    # use logistic regression instead of LR \n",
    "    logistic = True,\n",
    "    binary_vectorizer = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_scores = cb_model_scoring(logreg_params, anime_train, anime_description)\n",
    "logreg_scores_test_anime = cb_model_scoring(logreg_params, anime_test_animes, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(logreg_scores_test_anime >= 0.5).astype(int).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=assign_labels(anime_train, score_cutoff)['recommend'], y_pred=(logreg_scores >= 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=assign_labels(anime_test_animes, score_cutoff)['recommend'], y_pred=(logreg_scores_test_anime >= 0.5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4AOtfJFQP9x",
    "outputId": "ec9250af-8968-4d29-fded-8bbed9fab915"
   },
   "outputs": [],
   "source": [
    "cb_model_evaluation(logreg_params, anime_test, favorites_data, all_anime_data, cb_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "xps3UsRELdbn",
    "outputId": "2f7dc9b0-0f10-42f2-9d07-dc2731b6ae86"
   },
   "outputs": [],
   "source": [
    "# features with the most positive impact\n",
    "significant_features(logreg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HoTaW2hwN7Jq",
    "outputId": "af349aed-c6c3-49f2-d5f2-7f83147ba312"
   },
   "outputs": [],
   "source": [
    "# features with the most negative impact\n",
    "significant_features(logreg_params, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with TFIDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_config[\"model\"] = {\"C\": 0.0001, \n",
    "                      \"max_iter\": 1000, \n",
    "                      \"class_weight\": \"balanced\"\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAbkP0c_O8tw"
   },
   "outputs": [],
   "source": [
    "logreg_params = build_cb_model(\n",
    "    cb_config,\n",
    "    # extend dataset with class labels\n",
    "    gt_label,\n",
    "     # target 0/1 instead of raw scores\n",
    "    {**anime_description, **{'feedback': 'recommend'}},\n",
    "    # use logistic regression instead of LR \n",
    "    logistic = True,\n",
    "    binary_vectorizer = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_scores = cb_model_scoring(logreg_params, anime_train, anime_description)\n",
    "logreg_scores_test_anime = cb_model_scoring(logreg_params, anime_test_animes, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4AOtfJFQP9x",
    "outputId": "ec9250af-8968-4d29-fded-8bbed9fab915"
   },
   "outputs": [],
   "source": [
    "cb_model_evaluation(logreg_params, anime_test, favorites_data, all_anime_data, cb_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "xps3UsRELdbn",
    "outputId": "2f7dc9b0-0f10-42f2-9d07-dc2731b6ae86"
   },
   "outputs": [],
   "source": [
    "# features with the most positive impact\n",
    "significant_features(logreg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HoTaW2hwN7Jq",
    "outputId": "af349aed-c6c3-49f2-d5f2-7f83147ba312"
   },
   "outputs": [],
   "source": [
    "# features with the most negative impact\n",
    "significant_features(logreg_params, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omzbcLvoY9-H"
   },
   "source": [
    "# Content-based similarity models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build so called ***user profile***  - the weighted vector of user's preferences. This way, user represented as a vector of consolidated item features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TtGzITjftru"
   },
   "source": [
    "### User profile construction\n",
    "\n",
    "Assuming we have a user $u$, who gave ratings $r_i$ to each anime $i\\in\\mathcal{I_u}$, represented by feature vector $a_i$,  the corresponding user feature profile vector is then defined as:\n",
    "\n",
    "$$\n",
    "v_{\\mathcal{I}_u} = \\frac{\\sum_{i\\in\\mathcal{I_u}} (r_i \\cdot a_{i})}{\\sum_{i\\in\\mathcal{I_u}} r_i}\n",
    "$$\n",
    "\n",
    "In order to provide recommendations for the specific user we are going to compare an anime vector representation to user profile vector, which will indicate how close this anime to user preferences from $\\mathcal{I_u}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the other ways to construct a user profile? What could a the problem in the above formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-sA4590aQZ5"
   },
   "outputs": [],
   "source": [
    "def build_sim_model(config, trainset, trainset_description, binary_vectorizer=True):\n",
    "    _, word_vectorizer = generate_features(config, trainset, trainset_description, binary_vectorizer)\n",
    "    return None, word_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGHtFF-lB7NC"
   },
   "outputs": [],
   "source": [
    "sim_config = {\n",
    "    'vectorizer': cb_config['vectorizer'].copy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QUREf_ytB7Jz",
    "outputId": "c0964ad3-aaca-4d2c-a3af-60a50f502396"
   },
   "outputs": [],
   "source": [
    "sim_params = build_sim_model(sim_config, anime_train, anime_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation purpose we are going to measure distances between vector representation of user's favourites and the user's likes and dislikes. We assume that cosine similarity of 1st pair of vectors is greater than cosine similarity of 2nd pair of vectors. Accordingly, the distance functions in the evaluation of CR metric can be set as:\n",
    "$$\n",
    "\\text{dist}(k,\\mathcal{I_u^+}) = 1-\\text{sim}(v_{\\mathcal{I}_u^+}, v_k), \\\\\n",
    "\\text{dist}(k,\\mathcal{I_u^-}) = 1-\\text{sim}(v_{\\mathcal{I}_u^-}, v_k),\n",
    "$$\n",
    "where $v_{I_u^+}$ and $v_{I_u^-}$ are user feature profiles (consolidated based on features of liked and disliked items) and $v_k$ is a favorite item feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is cosine similarity better than dot product? What is be the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_distances(user_id, likes, dislikes, params, favorites, anime_features):\n",
    "    \"\"\"\n",
    "    Caclulate the distance between user's favorites and likes, \n",
    "    as well as between user's favorites and dislikes based on scores of a content-based similarity model\n",
    "    \"\"\"\n",
    "    _, word_vectorizer = params\n",
    "    # get favorite items features representation\n",
    "    # (n_favorites, vocabulary size)\n",
    "    favorites_features = word_vectorizer.transform(anime_features.loc[favorites[user_id], 'tokens'].values)\n",
    "    # get user representation\n",
    "    user_profile_pos = generate_feature_profile(likes, word_vectorizer, anime_features)\n",
    "    # calculate distance from liked items to favorites\n",
    "    similarity_pos = feature_similarity(favorites_features, user_profile_pos)\n",
    "    # calculate distance from disliked items to favorites\n",
    "    user_profile_neg = generate_feature_profile(dislikes, word_vectorizer, anime_features)\n",
    "    similarity_neg = feature_similarity(favorites_features, user_profile_neg)\n",
    "    return 1-similarity_pos, 1-similarity_neg\n",
    "        \n",
    "def generate_feature_profile(items, word_vectorizer, item_features):\n",
    "    \"\"\"\n",
    "    Feature profile of a user\n",
    "    \"\"\"\n",
    "    scores = item_features.loc[items, 'score'].values\n",
    "    tokens = item_features.loc[items, 'tokens'].values\n",
    "    # (n_items, vocabulary size)\n",
    "    feature_matrix = word_vectorizer.transform(tokens)\n",
    "    # (n_items,)\n",
    "    weights = scores / np.sum(scores)\n",
    "    # (vocabulary size, )\n",
    "    return feature_matrix.T.dot(weights)\n",
    "\n",
    "def feature_similarity(feature_matrix, feature_profile):\n",
    "    \"\"\"\n",
    "    Caclulate the similarity between user's favorites and likes/dislikes, \n",
    "    based on content-based cosine similarity\n",
    "    \"\"\"\n",
    "    # (n_favorites, vocabulary size) @ (vocabulary size)\n",
    "    similarity = feature_matrix.dot(feature_profile)\n",
    "    weights = sparse_norm(feature_matrix, axis=1) * np.linalg.norm(feature_profile)\n",
    "    similarity /= weights\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_model_evaluation(sim_params, anime_test, favorites_data, all_anime_data, sim_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: get the recommendation to the user by building the user's vector from his positives and finding the closest items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_user_positives  = anime_test.loc[\"King_Of_Light\", :][\"likes\"]\n",
    "the_user_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_data.query(\"anime_id in @the_user_positives\")[[\"anime_id\", \"title\", \"synopsis\", \"genre\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anime_data[\"tokens\"].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_data['anime_id'] == all_anime_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the user's vector\n",
    "# user_vector = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_item_feature_matrix = sim_params[1].transform(all_anime_data[\"tokens\"])\n",
    "full_item_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = sparse_norm(full_item_feature_matrix, axis=1) * np.linalg.norm(user_vector)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the scores\n",
    "# scores = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_user_pred = top_idx(scores, topk=10)\n",
    "\n",
    "\n",
    "pd.DataFrame({\"anime_idx\": the_user_pred, \"score\": scores[the_user_pred]}).merge(anime_data[[\"anime_id\", \"title\", \"synopsis\", \"genre\"]], left_on=\"anime_idx\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this evaluation by favorite genres completly fair? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
